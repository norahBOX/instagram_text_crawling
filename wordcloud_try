import konlpy
import pandas as pd
import numpy as np
 
from collections import Counter
 
from wordcloud import WordCloud
from wordcloud import STOPWORDS
 
import matplotlib.pyplot as plt
from PIL import Image

############################################################
from konlpy.tag import Kkma
tagger = Kkma()

##예시 example
print(tagger.pos("배고프지 않냐 진짜 라면 먹고 싶다."))  
#[('배고프', 'VA'), ('지', 'ECD'), ('않', 'VXV'), ('냐', 'EFQ'), ('진짜', 'MAG'), ('이', 'VCP'), ('라면', 'ECD'), ('먹', 'VV'), ('고', 'ECE'), ('싶', 'VXA'), ('다', 'EFN'), ('.', 'SF')]
print(tagger.nouns("배고프지 않냐 진짜 라면 먹고 싶다."))  
#[]
#라면이 명사로 분류되지 않는다.. ㅋㅋㅋㅋㅋ


############################################################


df = pd.read_csv('textmining.csv', encoding = 'utf-8-sig')
#이 textmining이라는 파일은 내가 앞에서 만든 코드로
#해시태그 '주머니시'를 검색해서 나온 포스트를
#모두 크롤링한 데이터임
#한 행당 포스트 하나의 글과 댓글들이 들어가 있음.


##1. df의 'post_text' 열(인스타그램 포스팅)
 
post_nouns = [] #명사 추출해서 담을 새 리스트
 
 
for i in range(len(df)) : 
    try : 
       # print(i)
        a = tagger.nouns(df['post_text'][i])
        post_nouns = post_nouns + a
    except : 
        pass #간혹가다 내용이 없는 포스트도 크롤링 되기 때문ㅋㅋ
 
 
count = Counter(post_nouns) #이거는 위에서 import한 모듈 중 Counter. 단어 개수 세어준다. 
 
key_post_150 = count.most_common(150) #빈도가 가장 많은 상위 150개의 단어를 추출 
key_post_150 = dict(key_post_150) #dictionary 형태로 만들어줌. 
 
 
## 그 와중 없앨 단어를 미리 모은다. 
## 주머니시 단어를 없앤 이유는.. 주머니시에 관한 워드 클라우드이기 때문...
stopwords = ['주머니시', '수', '분', '명', '저', '2',  '1', '우', '중', '3', '58', 
             '김', '이', '동', '4', '개', '송', '7', '인', 
             '등', '필', '때문', '송유수', '의', '번', '차', 
             '작', '전', '주',  '주신', '리',  ]
 
for i in stopwords : 
    key_post_150.pop(i)
#pop은 반복적으로 수행하면 에러 난다. 이미 없앤 거 또 없앨 수 없으니까....




wordcloud = WordCloud(font_path = "C:/Windows/Fonts/NanumBarunpenB.ttf", #원하는 폰트의 경로를 지정해준다. 안그럼 한글 안나오는 대참사  
                      background_color = 'white', width = 800, height = 600, 
                      stopwords = stopwords)
 
wordcloud = wordcloud.generate_from_frequencies(key_post_150)
#앞에서 만들어준 - 이상한 단어 다 뺀 상위 n개의 단어 dictionary - 을 넣는다. 
#참고로 generate_from_frequencies는 이와 같이 단어와 빈도수로 이루어진
#dictionary를 받아서 워드클라우드를 만들어주므로
#stopwords 옵션이 아무 소용이 없다. 


%matplotlib inline
import matplotlib.pyplot as plt
plt.figure(figsize = (10, 10))
plt.imshow(wordcloud, interpolation = 'bilinear')
plt.axis("off")
plt.show()

############################################################

##color changing
 
from wordcloud import get_single_color_func
 
class SimpleGroupedColorFunc(object):
    def __init__(self, color_to_words, default_color):
        self.word_to_color = {word: color
                              for (color, words) in color_to_words.items()
                              for word in words}
 
        self.default_color = default_color
 
    def __call__(self, word, **kwargs):
        return self.word_to_color.get(word, self.default_color)
    
 
color_to_words = {
    '#5B3173': ['감사'], #어떤 단어의 컬러를 어떻게 바꿀지 일일이 리스트로 지정
    '#ABB34E': ['글'] #물론 저렇게 html 컬러 코드만 되는 건 아니다.
#red, blue 같은 컬러 이름도 가능함
}
 
default_color = 'grey' #컬러 지정이 안 된 애들의 기본 컬러

simple_color_func = SimpleGroupedColorFunc(color_to_words, default_color)
wordcloud.recolor(color_func = simple_color_func)
plt.figure(figsize=(10, 10))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()


############################################################

##다른 모양으로 워드클라우드 만들기 (mask)


carrot_mask = np.array(Image.open("carrot-cut.png")) #이미지의 경로. 
 
wordcloud = WordCloud(font_path = "C:/Windows/Fonts/NanumBarunpenB.ttf", 
                      background_color='white', width = 800, height = 600,
                     mask = carrot_mask) #이렇게 mask를 추가 해주면 된다. 
    
wordcloud = wordcloud.generate_from_frequencies(keywords_150)
plt.figure(figsize=(10, 10))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()
